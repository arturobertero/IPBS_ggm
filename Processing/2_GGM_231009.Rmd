---
title: "3_IPBS_mgm_230912"
author: "Arturo Bertero"
date: "2023-09-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(tidyverse, here, sjlabelled, stringr, glue, EGAnet, janitor, haven,
       listviewerlite, ggpubr, gridExtra, dplyr, GGally, qgraph, sjmisc, igraph,
       sjPlot, sna, grid, psych, stargazer, mgm, backbone, ggplot2, tnet,
       ggrepel, EGAnet, NetworkComparisonTest, Matrix, bootnet, matrixcalc, 
       openxlsx, devtools, jtools, corrr, patchwork)


```

# Input

```{r}
#Load data
IPBS = readRDS((here("Input", "IPBS.rds")))

#Filter smaller datasets
PTVs = IPBS %>% 
  dplyr::select(c(PTV_PD:PTV_FDI))

PTV_LR = IPBS %>% 
  dplyr::select(c(L_R:PTV_FDI))

att = IPBS %>% 
  dplyr::select(c(L_R:weapons_ukr)) 

all = IPBS %>% 
  dplyr::select(c(L_R:vote))

#Partitions
load(file = here("Input", "Partitions", "pol_int_partitions.RData"))
load(file = here("Input", "Partitions", "pol_int_partitions_2_1.RData"))
load(file = here("Input", "Partitions", "pol_int_partitions_2_2.RData"))
load(file = here("Input", "Partitions", "educ_partitions.RData"))

#Fast running (run [0] or skip [1] bootnet)
fast_running = 1
```

# Processing

##  Basic GGM estimation
```{r}
#Objects
shortnames = names(att)
longnames = c("Left right","Propensity to vote for PD",
              "Propensity to vote for FI","Propensity to vote for L",
              "Propensity to vote for M5S","Propensity to vote for FDI",
              "Step child adoption","Abortion","Euthanasia","Omosexual marriage",
              "Redistribution","Flat tax","Minimum wage","Citizenship income",
              "Globalization","Immigration","Big government",
              "Public vs private","Weapons to Ukraine")

Totalgroup_comm <- list(
 "Symbolic"=c(1:6),
 "Operational"=c(7:19))

Totalgroup_cols <- c("#E96479","#3FA796")

#remove labels
att = sapply(att, haven::zap_labels)

#declare operational or symbolic
nature = c(rep("Symbolic", 6),
            rep("Operational", 13))

#Fit GGM att_net
att_net = cor_auto(att, npn.SKEPTIC = T, #Nonparanormal transformation (HUGE)
                        ordinalLevelMax = 7, #from keskinturk
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE)

att_net = EBICglasso(S = att_net, n = nrow(att))

#plot
set.seed(1)
Graph_att = qgraph(att_net,
  layout = "spring", theme = "Borkulo", 
  labels = shortnames, nodeNames = longnames,
  cut = 0.07, maximum = 1, vsize=7.0, label.cex=1.1,
  groups=Totalgroup_comm, color=Totalgroup_cols,
  legend = TRUE, legend.cex = 0.40, borders = FALSE,
  filetype="jpg", filename=here("Output", "GGM", "mgm_att"))
```

## Estimation by levels of polint
```{r}
#data
pint_l_data = sapply(polint_low_and_midlow, haven::zap_labels) 
pint_m_data = sapply(pol_int_midhigh , haven::zap_labels)

#Gather pint_l and pint_m into "pint_l"
pint_l_data = rbind(pint_l_data,pint_m_data)
pint_h_data = sapply(pol_int_high, haven::zap_labels)

#Fit GGM pint_l_data
pint_l_data_net = cor_auto(pint_l_data, npn.SKEPTIC = T, 
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE)

pint_l_data_net = EBICglasso(S = pint_l_data_net, n = nrow(pint_l_data))

#plot
#set.seed(1)
#Graph_pint_l = qgraph(pint_l_data_net,
#  layout = "spring", theme = "Borkulo", 
#  labels = shortnames, nodeNames = longnames,
#  cut = 0.07, maximum = 1, vsize=7.0, label.cex=1.1,
#  groups=Totalgroup_comm, color=Totalgroup_cols,
#  legend = TRUE, legend.cex = 0.40, borders = FALSE,
#  filetype="jpg", filename=here("Output", "GGM", "ggm_pint_l"))

#Fit GGM pint_h_data
pint_h_data_net = cor_auto(pint_h_data, npn.SKEPTIC = T, 
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE)

pint_h_data_net = EBICglasso(S = pint_h_data_net, n = nrow(pint_h_data))

#plot
#set.seed(1)
#Graph_pint_h = qgraph(pint_h_data_net,
#  layout = "spring", theme = "Borkulo", 
#  labels = shortnames, nodeNames = longnames,
#  cut = 0.07, maximum = 1, vsize=7.0, label.cex=1.1,
#  groups=Totalgroup_comm, color=Totalgroup_cols,
#  legend = TRUE, legend.cex = 0.40, borders = FALSE,
#  filetype="jpg", filename=here("Output", "GGM", "ggm_pint_h"))
```

## Estimation by levels of educ
```{r}
#data
educ_l_data = sapply(educ_low, haven::zap_labels) 
educ_h_data = sapply(educ_high , haven::zap_labels)

#Fit GGM educ_l_data
educ_l_data_net = cor_auto(educ_l_data, npn.SKEPTIC = T, 
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE)

educ_l_data_net = EBICglasso(S = educ_l_data_net, n = nrow(educ_l_data))

#plot
#set.seed(1)
#Graph_educ_l = qgraph(educ_l_data_net,
#  layout = "spring", theme = "Borkulo", 
#  labels = shortnames, nodeNames = longnames,
#  cut = 0.07, maximum = 1, vsize=7.0, label.cex=1.1,
#  groups=Totalgroup_comm, color=Totalgroup_cols,
#  legend = TRUE, legend.cex = 0.40, borders = FALSE,
#  filetype="jpg", filename=here("Output", "GGM", "ggm_educ_l"))

#Fit GGM educ_h_data
educ_h_data_net = cor_auto(educ_h_data, npn.SKEPTIC = T, 
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE)

educ_h_data_net = EBICglasso(S = educ_h_data_net, n = nrow(educ_h_data))

#plot
#set.seed(1)
#Graph_educ_h = qgraph(educ_h_data_net,
#  layout = "spring", theme = "Borkulo", 
#  labels = shortnames, nodeNames = longnames,
#  cut = 0.07, maximum = 1, vsize=7.0, label.cex=1.1,
#  groups=Totalgroup_comm, color=Totalgroup_cols,
#  legend = TRUE, legend.cex = 0.40, borders = FALSE,
#  filetype="jpg", filename=here("Output", "GGM", "ggm_educ_h"))
```

## Network with behavior
```{r}
#Objects
shortnames_all = names(all)
longnames_all = c("Left right","Propensity to vote for PD",
              "Propensity to vote for FI","Propensity to vote for L",
              "Propensity to vote for M5S","Propensity to vote for FDI",
              "Step child adoption","Abortion","Euthanasia","Omosexual marriage",
              "Redistribution","Flat tax","Minimum wage","Citizenship income",
              "Globalization","Immigration","Big government",
              "Public vs private","Weapons to Ukraine","Vote")
Totalgroup_comm_all <- list(
 "Symbolic"=c(1:6),
 "Operational"=c(7:19),
 "Behavior"= 20)
Totalgroup_cols_all <- c("#E96479","#3FA796","#C3ACD0")

#data
all = sapply(all, haven::zap_labels) 

#declare operational or symbolic (H2)
nature_all = c(rep("Symbolic", 6),
            rep("Operational", 13),
            "Vote")

#Fit GGM att_net
all_net = cor_auto(all, npn.SKEPTIC = T, #Nonparanormal transformation (HUGE)
                        ordinalLevelMax = 7, #from keskinturk
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE)

all_net = EBICglasso(S = all_net, n = nrow(all))

#plot
set.seed(1)
Graph_all = qgraph(all_net,
  layout = "spring", theme = "Borkulo", 
  labels = shortnames_all, nodeNames = longnames_all,
  cut = 0.07, maximum = 1, vsize=7.0, label.cex=1.1,
  groups=Totalgroup_comm_all, color=Totalgroup_cols_all,
  legend = TRUE, legend.cex = 0.40, borders = FALSE,
  filetype="jpg", filename=here("Output", "GGM", "mgm_all"))

```

## Bootstrap analyses

```{r}
if (fast_running==0) {

##############################
#Bootstrapped on full samples
##############################

#boots_att
boot_att = zap_labels(att) %>%
          data.frame()
df.boot_att = bootnet(boot_att, nBoots = 10000, nCores = 8,
                     default = "EBICglasso", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)
#Save Bootnets
save(df.boot_att, file = here("Input", "Boots", "boots_att.RData"))

#boots_all
boot_all = zap_labels(all) %>%
          data.frame()
df.boot_all = bootnet(boot_all, nBoots = 10000, nCores = 8,
                     default = "EBICglasso", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)
#Save Bootnets
save(df.boot_all, file = here("Input", "Boots", "boots_all.RData"))

###############
#Bootstrap pint
###############

#low
polint_low_and_midlow = zap_labels(polint_low_and_midlow) %>%
          data.frame()
df.boot_pint_l <- bootnet(polint_low_and_midlow, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#mid
pol_int_midhigh = zap_labels(pol_int_midhigh) %>%
          data.frame()
df.boot_pint_m <- bootnet(pol_int_midhigh, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#high
pol_int_high = zap_labels(pol_int_high) %>%
          data.frame()
df.boot_pint_h <- bootnet(pol_int_high, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#Save Bootnets
save(df.boot_pint_l,df.boot_pint_m,df.boot_pint_h,
     file = here("Input", "Boots", "boots_pint.RData"))

###############
#Bootstrap educ
###############

#low
educ_low = zap_labels(educ_low) %>%
          data.frame()
df.boot_educ_l <- bootnet(educ_low, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#high
educ_high = zap_labels(educ_high) %>%
          data.frame()
df.boot_educ_h <- bootnet(educ_high, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#Save Bootnets
save(df.boot_educ_l,df.boot_educ_h,
     file = here("Input", "Boots", "boots_educ.RData"))

#################
#Additional pint#
#################

# On 2_1

#low
polint_l_2_1 = zap_labels(polint_2_1_l) %>%
          data.frame()
df.boot_pint_l_2_1 <- bootnet(polint_l_2_1, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#high
polint_h_2_1 = zap_labels(polint_2_1_h) %>%
          data.frame()
df.boot_pint_h_2_1 <- bootnet(polint_h_2_1, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#Save Bootnets
save(df.boot_pint_l_2_1,df.boot_pint_h_2_1,
     file = here("Input", "Boots", "boots_pint_2_1.RData"))

# On 2_2

#low
polint_l_2_2 = zap_labels(polint_2_2_l) %>%
          data.frame()
df.boot_pint_l_2_2 <- bootnet(polint_l_2_2, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#high
polint_h_2_2 = zap_labels(polint_2_2_h) %>%
          data.frame()
df.boot_pint_h_2_2 <- bootnet(polint_h_2_2, nBoots = 10000, nCores = 8,
                     default = "cor", type = "nonparametric", model = "GGM",
                     corMethod = "cor_auto", corArgs = list(
                        npn.SKEPTIC = T,
                        ordinalLevelMax = 7, 
                        forcePD = TRUE,
                        missing = "pairwise",
                        verbose = FALSE),
                     statistics = c("strength", "bridgeCloseness", 
                     "bridgeBetweenness"), computeCentrality = TRUE)

#Save Bootnets
save(df.boot_pint_l_2_2,df.boot_pint_h_2_2,
     file = here("Input", "Boots", "boots_pint_2_2.RData"))

} else {
  
  #Boots
load(file = here("Input", "Boots", "boots_att.RData"))
load(file = here("Input", "Boots", "boots_all.RData"))
load(file = here("Input", "Boots", "boots_pint.RData"))
load(file = here("Input", "Boots", "boots_educ.RData"))
load(file = here("Input", "Boots", "boots_pint_2_1.RData"))
load(file = here("Input", "Boots", "boots_pint_2_2.RData"))
}

#More boots

```

### H1a, constraint hypothesis (cor network)
The network of people with high political interest will have higher constraint 
if compared to the one estimated on the low political interest sample.

```{r}
#Extract boots_pint_l

# Create an empty list to store the matrices
matrix_list <- list()

# Loop through from 1 to 10000
for (i in 1:10000) {
  # Extract the matrix and add it to the list
  matrix_list[[i]] <- df.boot_pint_l[["boots"]][[i]][["graph"]]
}

# Create an empty vector to store the mean values
mean_values <- numeric(10000)

# Loop through each matrix in matrix_list
for (i in 1:10000) {
  # Square each cell of the matrix
  squared_matrix <- matrix_list[[i]]^2
  
  # Calculate the mean of the squared matrix
  mean_value <- mean(squared_matrix)
  
  # Store the mean value in the vector
  mean_values[i] <- mean_value
}

cons_pint_l = mean_values
```

```{r}
#Loop: extract boots_pint_m

matrix_list <- list()

for (i in 1:10000) {
  matrix_list[[i]] <- df.boot_pint_m[["boots"]][[i]][["graph"]]
}

mean_values <- numeric(10000)
for (i in 1:10000) {
  squared_matrix <- matrix_list[[i]]^2
  mean_value <- mean(squared_matrix)
  mean_values[i] <- mean_value
}

cons_pint_m = mean_values
```

```{r}
#Loop: extract boots_pint_h

matrix_list <- list()

for (i in 1:10000) {
  matrix_list[[i]] <- df.boot_pint_h[["boots"]][[i]][["graph"]]
}

mean_values <- numeric(10000)
for (i in 1:10000) {
  squared_matrix <- matrix_list[[i]]^2
  mean_value <- mean(squared_matrix)
  mean_values[i] <- mean_value
}

cons_pint_h = mean_values
```

```{r}
#Graph H1a
# Create a vector pint
pint = c(rep("Low", 10000), rep("Mid", 10000), rep("High", 10000))
pint = factor(pint, levels=c("Low", "Mid", "High"))
# Create mcon_pint for the graph
mcon_pint <- data.frame(
  const_val = c(cons_pint_l,cons_pint_m,cons_pint_h),
  pint = pint
)

# Calculate quantile-based confidence intervals (90% CI)
mcon_pint_summary <- mcon_pint %>%
  group_by(pint) %>%
  summarise(
    m = mean(const_val),
    ymin = quantile(const_val, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(const_val, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

# Create the plot
H1a <- ggplot() +
  geom_violin(data = mcon_pint, aes(x = pint, y = const_val), 
              scale = "count", width = 0.6, alpha = 0.5, fill = "black") +
  geom_errorbar(data = mcon_pint_summary, aes(x = pint, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  ylab("Average Constraint") +
  xlab("Political Interest") +
  theme(axis.title.x = element_text(), legend.position = "none")

# Save the plot
ggsave(here("Output", "GGM", "H1a.jpg"), H1a, width = 6, height = 4)

``` 

### H1a 2_1
Replicate on pint with two levels to assess stability
```{r}
#Extract boots_pint_l_2_1

# Create an empty list to store the matrices
matrix_list <- list()

# Loop through from 1 to 10000
for (i in 1:10000) {
  # Extract the matrix and add it to the list
  matrix_list[[i]] <- df.boot_pint_l_2_1[["boots"]][[i]][["graph"]]
}

# Create an empty vector to store the mean values
mean_values <- numeric(10000)

# Loop through each matrix in matrix_list
for (i in 1:10000) {
  # Square each cell of the matrix
  squared_matrix <- matrix_list[[i]]^2
  
  # Calculate the mean of the squared matrix
  mean_value <- mean(squared_matrix)
  
  # Store the mean value in the vector
  mean_values[i] <- mean_value
}

cons_pint_l_2_1 = mean_values
```

```{r}
#Loop: extract boots_pint_h_2_1

matrix_list <- list()

for (i in 1:10000) {
  matrix_list[[i]] <- df.boot_pint_h_2_1[["boots"]][[i]][["graph"]]
}

mean_values <- numeric(10000)
for (i in 1:10000) {
  squared_matrix <- matrix_list[[i]]^2
  mean_value <- mean(squared_matrix)
  mean_values[i] <- mean_value
}

cons_pint_h_2_1 = mean_values
```

```{r}
#Graph H1a_2_1
# Create a vector pint
pint = c(rep("Low", 10000), rep("High", 10000))
pint = factor(pint, levels=c("Low", "High"))
# Create mcon_pint for the graph
mcon_pint_2_1 <- data.frame(
  const_val = c(cons_pint_l_2_1,cons_pint_h_2_1),
  pint = pint
)

# Calculate quantile-based confidence intervals (90% CI)
mcon_pint_2_1_summary <- mcon_pint_2_1 %>%
  group_by(pint) %>%
  summarise(
    m = mean(const_val),
    ymin = quantile(const_val, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(const_val, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

# Create the plot
H1a_2_1 <- ggplot() +
  geom_violin(data = mcon_pint_2_1, aes(x = pint, y = const_val), 
              scale = "count", width = 0.6, alpha = 0.5, fill = "black") +
  geom_errorbar(data = mcon_pint_2_1_summary, aes(x = pint, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  ylab("Average Constraint") +
  xlab("Political Interest") +
  theme(axis.title.x = element_text(), legend.position = "none")

# Save the plot
ggsave(here("Output", "GGM", "H1a_2_1.jpg"), H1a_2_1, width = 6, height = 4)

``` 

### H1a 2_2
Replicate on pint with two levels to assess stability
```{r}
#Extract boots_pint_l_2_2

# Create an empty list to store the matrices
matrix_list <- list()

# Loop through from 1 to 10000
for (i in 1:10000) {
  # Extract the matrix and add it to the list
  matrix_list[[i]] <- df.boot_pint_l_2_2[["boots"]][[i]][["graph"]]
}

# Create an empty vector to store the mean values
mean_values <- numeric(10000)

# Loop through each matrix in matrix_list
for (i in 1:10000) {
  # Square each cell of the matrix
  squared_matrix <- matrix_list[[i]]^2
  
  # Calculate the mean of the squared matrix
  mean_value <- mean(squared_matrix)
  
  # Store the mean value in the vector
  mean_values[i] <- mean_value
}

cons_pint_l_2_2 = mean_values
```

```{r}
#Loop: extract boots_pint_h_2_2

matrix_list <- list()

for (i in 1:10000) {
  matrix_list[[i]] <- df.boot_pint_h_2_2[["boots"]][[i]][["graph"]]
}

mean_values <- numeric(10000)
for (i in 1:10000) {
  squared_matrix <- matrix_list[[i]]^2
  mean_value <- mean(squared_matrix)
  mean_values[i] <- mean_value
}

cons_pint_h_2_2 = mean_values
```

```{r}
#Graph H1a_2_2
# Create a vector pint
pint = c(rep("Low", 10000), rep("High", 10000))
pint = factor(pint, levels=c("Low", "High"))
# Create mcon_pint for the graph
mcon_pint_2_2 <- data.frame(
  const_val = c(cons_pint_l_2_2,cons_pint_h_2_2),
  pint = pint
)

# Calculate quantile-based confidence intervals (90% CI)
mcon_pint_2_2_summary <- mcon_pint_2_2 %>%
  group_by(pint) %>%
  summarise(
    m = mean(const_val),
    ymin = quantile(const_val, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(const_val, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

# Create the plot
H1a_2_2 <- ggplot() +
  geom_violin(data = mcon_pint_2_2, aes(x = pint, y = const_val), 
              scale = "count", width = 0.6, alpha = 0.5, fill = "black") +
  geom_errorbar(data = mcon_pint_2_2_summary, aes(x = pint, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  ylab("Average Constraint") +
  xlab("Political Interest") +
  theme(axis.title.x = element_text(), legend.position = "none")

# Save the plot
ggsave(here("Output", "GGM", "H1a_2_2.jpg"), H1a_2_2, width = 6, height = 4)

``` 

### H1b, rival constraint hypothesis (cor network)
The network of people with high education will have higher connectivity if 
compared to the one estimated on the low education sample.

```{r}
#Loop: extract boots_educ_l

matrix_list <- list()

for (i in 1:10000) {
  matrix_list[[i]] <- df.boot_educ_l[["boots"]][[i]][["graph"]]
}

mean_values <- numeric(10000)
for (i in 1:10000) {
  squared_matrix <- matrix_list[[i]]^2
  mean_value <- mean(squared_matrix)
  mean_values[i] <- mean_value
}

cons_educ_l = mean_values
```


```{r}
#Loop: extract boots_educ_h

matrix_list <- list()

for (i in 1:10000) {
  matrix_list[[i]] <- df.boot_educ_h[["boots"]][[i]][["graph"]]
}

mean_values <- numeric(10000)
for (i in 1:10000) {
  squared_matrix <- matrix_list[[i]]^2
  mean_value <- mean(squared_matrix)
  mean_values[i] <- mean_value
}

cons_educ_h = mean_values
```

```{r}
#Graph H1b
# Create a vector educ
educ = c(rep("Low", 10000), rep("High", 10000))
educ = factor(educ, levels=c("Low", "High"))
# Create mcon_educ for the graph
mcon_educ <- data.frame(
  const_val = c(cons_educ_l,cons_educ_h),
  educ = educ
)

# Calculate quantile-based confidence intervals (90% CI)
mcon_edu_summary <- mcon_educ %>%
  group_by(educ) %>%
  summarise(
    m = mean(const_val),
    ymin = quantile(const_val, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(const_val, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

# Create the plot
H1b <- ggplot() +
  geom_violin(data = mcon_educ, aes(x = educ, y = const_val), 
              scale = "count", width = 0.6, alpha = 0.5, fill = "black") +
  geom_errorbar(data = mcon_edu_summary, aes(x = educ, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  ylab("Average Constraint") +
  xlab("Education") +
  theme(axis.title.x = element_text(), legend.position = "none")

ggsave(here("Output", "GGM", "H1b.jpg"), H1b, width = 6, height = 4) 

```

### H2, centrality hypothesis
The mean centrality of symbolic variables will be higher than the mean 
centrality of operational variables.

```{r}
# Strength data from the boots
boots_att_strength = df.boot_att$bootTable %>% 
  filter(type=="strength") %>% 
  mutate(type = c(rep(nature, 10000))) %>% 
  dplyr::select(c("value", "type"))
  
# Betweenness data from the boots
boots_att_betweenness = df.boot_att$bootTable %>% 
  filter(type=="bridgeBetweenness") %>% 
  mutate(type = c(rep(nature, 10000))) %>% 
  dplyr::select(c("value", "type"))

# Closeness data from the boots
boots_att_closeness = df.boot_att$bootTable %>% 
  filter(type=="bridgeCloseness") %>% 
  mutate(type = c(rep(nature, 10000))) %>% 
  dplyr::select(c("value", "type")) %>% 
  mutate(value=value*100)  #multiplied for 100 as in Brandt
```

```{r}
# Calculate quantile-based confidence intervals (90% CI)

#strength
strength_summary <- boots_att_strength %>%
  group_by(type) %>%
  summarise(
    m = mean(value),
    ymin = quantile(value, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(value, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

#betweenness
betweenness_summary <- boots_att_betweenness %>%
  group_by(type) %>%
  summarise(
    m = mean(value),
    ymin = quantile(value, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(value, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

#closeness
closeness_summary <- boots_att_closeness %>%
  group_by(type) %>%
  summarise(
    m = mean(value),
    ymin = quantile(value, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(value, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )
```


```{r}
#Strength
H2_strength = ggplot() +
  geom_violin(data = boots_att_strength, 
              aes(x = type, y = value, fill = type, color = type), 
              scale = "count", width = 0.6, alpha = 0.5, color = "transparent") +
  geom_errorbar(data = strength_summary, aes(x = type, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  xlab(NULL) +  
  ylab(NULL) +
  scale_color_manual(values = c("Operational" = "#3FA796", "Symbolic" = "#E96479")) +
  guides(fill = FALSE, color = FALSE) +
  ggtitle("Strength")
#save this one for the paper
ggsave(here("Output", "GGM", "H2_strength.jpg"), H2_strength, width = 6, height = 4)
```

```{r}
#betweenness
H2_betweenness = ggplot() +
  geom_violin(data = boots_att_betweenness, 
              aes(x = type, y = value, fill = type, color = type), 
              scale = "count", width = 0.6, alpha = 0.5, color = "transparent") +
  geom_errorbar(data = betweenness_summary, aes(x = type, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  xlab(NULL) +  
  ylab(NULL) +
  scale_color_manual(values = c("Operational" = "#3FA796", "Symbolic" = "#E96479")) +
  guides(fill = FALSE, color = FALSE) +
  ggtitle("Betweenness")

#ggsave(here("Output", "GGM", "H2_betweenness.jpg"), H2_betweenness, width = 6, height = 4)
```

```{r}
#Closeness
H2_closeness = ggplot() +
  geom_violin(data = boots_att_closeness, 
              aes(x = type, y = value, fill = type, color = type), 
              scale = "count", width = 0.6, alpha = 0.5, color = "transparent") +
  geom_errorbar(data = closeness_summary, aes(x = type, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  xlab(NULL) +  
  ylab(NULL) +
  scale_color_manual(values = c("Operational" = "#3FA796", "Symbolic" = "#E96479")) +
  guides(fill = FALSE, color = FALSE) +
  ggtitle("Closeness")

#ggsave(here("Output", "GGM", "H2_closeness.jpg"), H2_closeness, width = 6, height = 4)
```

```{r}
#multiplot
H2_multiplot = H2_strength + H2_betweenness + H2_closeness 
ggsave(here("Output", "GGM", "H2_multiplot.jpg"), H2_multiplot, width = 8, height = 6)
```

### H3, vote hypothesis
The node vote will be more closely connected to symbolic, rather than 
operational, variables

```{r}
#Data mang for graph H3

# Create an empty list to store the matrices
matrix_list <- list()

# Loop through from 1 to 1000
for (i in 1:10000) {
  # Extract the matrix and add it to the list
  matrix_list[[i]] <- df.boot_all[["boots"]][[i]][["graph"]]
}

# Create an empty data frame to store the results
result_df <- data.frame()

# Loop through each matrix in matrix_list
for (i in 1:10000) {
  centrality_result <- centrality(
    matrix_list[[i]], all.shortest.paths = TRUE)$ShortestPathLengths[, 20]
  
  # Create a temporary data frame for this iteration
  temp_df <- data.frame(
    voteshort = centrality_result,
    type = nature_all,
    node = shortnames_all
  )
  
  # Append the temporary data frame to the result_df
  result_df <- rbind(result_df, temp_df)
}

#Data from results_df
data_h3_boot = result_df %>% 
  dplyr::select(c(voteshort, type)) %>% 
  dplyr::filter(type != "Vote")

#as factor
data_h3_boot$type = as.factor(data_h3_boot$type)
```


```{r}
#strength
vote_summary <- data_h3_boot %>%
  group_by(type) %>%
  summarise(
    m = mean(voteshort),
    ymin = quantile(voteshort, 0.025),  # 2.5 percentile, α=0.05 (α/2 = 0.025)
    ymax = quantile(voteshort, 0.975)   # 97.5 percentile, α=0.05 (1 - α/2 = 0.975)
  )

#Strength
H3_boot = ggplot() +
  geom_violin(data = data_h3_boot, 
              aes(x = type, y = voteshort, fill = type, color = type), 
              scale = "count", width = 0.6, alpha = 0.5, color = "transparent") +
  geom_errorbar(data = vote_summary, aes(x = type, ymin = ymin, ymax = ymax), 
                width = 0.2, linewidth = 0.7, color = "black") +
  xlab(NULL) +  
  ylab(NULL) +
  scale_color_manual(values = c("Operational" = "#3FA796", "Symbolic" = "#E96479")) +
  guides(fill = FALSE, color = FALSE)

ggsave(here("Output", "GGM", "H3_boot.jpg"), H3_boot, width = 6, height = 4)  
```



